{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10a5427",
   "metadata": {},
   "source": [
    "# 回归分析 Regression Analysis\n",
    "\n",
    "xyfJASON\n",
    "\n",
    "在「插值与拟合」中我们已经了解了最小二乘法作曲线拟合，但是从数理统计的观点看，数据点的观测具有误差，可以视为**随机变量**，我们有必要对结果作区间估计或假设检验，以评估结果的可信度和模型的优劣。**简单地说，回归分析就是对拟合问题作统计分析。**\n",
    "\n",
    "回归分析会研究以下几个问题：\n",
    "1. 建立因变量 $y$ 和自变量 $x_1,x_2,\\ldots,x_m$ 之间的回归模型；\n",
    "2. 检验回归模型的可信度（拟合效果）；\n",
    "3. 检验每个自变量 $x_i$ 对 $y$ 的影响是否显著；\n",
    "4. 判断回归模型是否适合样本数据；\n",
    "5. 使用回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad062018",
   "metadata": {},
   "source": [
    "## 1 一元线性回归\n",
    "\n",
    "### 1.1 模型\n",
    "\n",
    "一元线性回归的模型为：\n",
    "\n",
    "$$\n",
    "y=\\beta_0+\\beta_1x+\\epsilon\n",
    "$$\n",
    "\n",
    "其中 $\\beta_0,\\beta_1$ 是回归系数，$\\epsilon\\sim N(0, \\sigma^2)$ 是随机误差项，故随机变量 $y\\sim N(\\beta_0+\\beta_1x, \\sigma^2)$。\n",
    "\n",
    "设我们进行了 $n$ 次观测，得到样本 $\\{x_i,y_i\\},\\,i=1,2,\\ldots,n$，它们符合模型：$y_i=\\beta_0+\\beta_1x_i+\\epsilon_i$，且 $\\epsilon_i$ 之间相互独立。\n",
    "\n",
    "### 1.2 最小二乘估计\n",
    "\n",
    "#### 1.2.1 最小二乘法\n",
    "\n",
    "最小二乘估计即取 $\\beta_0,\\beta_1$ 的一组估计值 $\\hat\\beta_0,\\hat\\beta_1$，使得误差平方和最小：\n",
    "\n",
    "$$\n",
    "\\hat\\beta_0,\\hat\\beta_1=\\arg\\min_{\\beta_0,\\beta_1}\\sum_{i=1}^n\\left(y_i-\\beta_0-\\beta_1x_i\\right)^2\n",
    "$$\n",
    "\n",
    "令偏导为零，解方程可得：\n",
    "\n",
    "$$\n",
    "\\hat\\beta_1=\\frac{\\sum\\limits_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)}{\\sum\\limits_{i=1}^n(x_i-\\bar x)^2},\\quad\\hat\\beta_0=\\bar y-\\hat\\beta_1\\bar x\n",
    "$$\n",
    "\n",
    "其中 $\\bar x=\\frac{1}{n}\\sum\\limits_{i=1}^nx_i,\\,\\bar y=\\frac{1}{n}\\sum\\limits_{i=1}^ny_i$。\n",
    "\n",
    "也可以改写为：\n",
    "\n",
    "$$\n",
    "\\hat\\beta_1=\\frac{s_y}{s_x}r_{xy}\n",
    "$$\n",
    "\n",
    "其中 $s_x^2=\\frac{1}{n-1}\\sum\\limits_{i=1}^n(x_i-\\bar x)^2,\\,s_y^2=\\frac{1}{n-1}\\sum\\limits_{i=1}^n(y_i-\\bar y)^2$ 是样本方差，$r_{xy}=\\cfrac{\\sum\\limits_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)}{\\sqrt{\\sum\\limits_{i=1}^n(x_i-\\bar x)^2}\\sqrt{\\sum\\limits_{i=1}^n(y_i-\\bar y)^2}}$ 是 $x$ 与 $y$ 的样本相关系数。\n",
    "\n",
    "特别地，当 $x,y$ 均已标准化时，$\\bar x=\\bar y=0,\\,s_x=s_y=1$，于是回归方程为：\n",
    "\n",
    "$$\n",
    "\\hat y=r_{xy}x\n",
    "$$\n",
    "\n",
    "#### 1.2.2 $\\hat\\beta_1$ 的性质\n",
    "\n",
    "注意 $\\hat\\beta_1$ 是一个随机变量，它具有以下性质：\n",
    "1. $\\hat\\beta_1$ 可以写作 $y_i$ 的线性组合，即 $\\hat\\beta_1=\\sum\\limits_{i=1}^nk_iy_i$，其中 $k_i=\\cfrac{x_i-\\bar x}{\\sum\\limits_{j=1}^n(x_j-\\bar x)^2}$；\n",
    "2. 由于 $y_i$ 是相互独立的正态随机变量，所以 $\\hat\\beta_1$ 也是正态随机变量；\n",
    "3. 点估计量 $\\hat\\beta_1$ 是真值 $\\beta_1$ 的无偏估计，即 $\\mathbb E[\\hat \\beta_1]=\\beta_1$；\n",
    "4. 点估计量 $\\hat\\beta_1$ 的方差为：$\\text{var}(\\hat\\beta_1)=\\cfrac{\\sigma^2}{\\sum\\limits_{i=1}^n(x_i-\\bar x)^2}$\n",
    "\n",
    "#### 1.2.3 其他性质\n",
    "\n",
    "最小二乘还具有一些值得注意的性质：\n",
    "1. 残差和为零：$\\sum\\limits_{i=1}^ne_i=\\sum\\limits_{i=1}^n(y_i-\\hat y_i)=0$\n",
    "2. 拟合值 $\\hat y_i$ 的平均值等于观测值 $y_i$ 的平均值：$\\frac{1}{n}\\sum\\limits_{i=1}^n\\hat y_i=\\frac{1}{n}\\sum\\limits_{i=1}^n y_i=\\bar y$\n",
    "3. $\\sum\\limits_{i=1}^nx_ie_i=0$\n",
    "4. $\\sum\\limits_{i=1}^n\\hat y_ie_i=0$\n",
    "5. 回归直线总是过 $(\\bar x, \\bar y)$\n",
    "\n",
    "\n",
    "\n",
    "### 1.3 拟合效果分析\n",
    "\n",
    "#### 1.3.1 残差的样本方差\n",
    "\n",
    "残差：$e_i=y_i-\\hat y_i$，其样本均值为：$\\frac{1}{n}\\sum\\limits_{i=1}^n(y_i-\\hat y_i)=0$，其样本方差（也即均方误差）为：\n",
    "\n",
    "$$\n",
    "\\text{MSE}=\\frac{1}{n-2}\\sum_{i=1}^ne_i^2=\\frac{1}{n-2}\\sum_{i=1}^n(y_i-\\hat y_i)^2\n",
    "$$\n",
    "\n",
    "（由于有两个约束：$\\sum\\limits_{i=1}^ne_i=0,\\,\\sum\\limits_{i=1}^nx_ie_i=0$，所以自由度为 $n-2$）$\\text{MSE}$ 是总体方差 $\\sigma^2=\\text{var}(\\epsilon_i)$ 的无偏估计量。\n",
    "\n",
    "#### 1.3.2 判定系数\n",
    "\n",
    "不同的 $x_i$ 对应不同的 $y_i$，建立一元线性回归模型，就是试图用 $x$ 的线性函数解释 $y$ 的变异。因此我们需要判定回归模型 $\\hat y=\\hat\\beta_1x+\\hat \\beta_0$ 究竟能以多大精度解释 $y$ 的变异。\n",
    "\n",
    "$y$ 的变异可以由样本方差刻画：\n",
    "\n",
    "$$\n",
    "s^2=\\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\bar y)^2\n",
    "$$\n",
    "\n",
    "根据前述性质，拟合值 $\\hat y_i$ 的均值也是 $\\bar y$，故其变异程度可以类似地刻画：\n",
    "\n",
    "$$\n",
    "\\hat s^2=\\frac{1}{n-1}\\sum_{i=1}^n(\\hat y_i-\\bar y)^2\n",
    "$$\n",
    "\n",
    "上述二者的关系是：\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n(y_i-\\bar y)^2=\\sum_{i=1}^n(\\hat y_i-\\bar y)^2+\\sum_{i=1}^n(y_i-\\hat y_i)^2\n",
    "$$\n",
    "\n",
    "上式中第一项 $\\text{SST}=\\sum\\limits_{i=1}^n(y_i-\\bar y)^2$ 是原始数据的变异程度；第二项 $\\text{SSR}=\\sum\\limits_{i=1}^n(\\hat y_i-\\bar y)^2$ 是拟合数据的变异程度；第三项 $\\text{SSE}=\\sum\\limits_{i=1}^n(y_i-\\hat y_i)^2$ 是残差平方和。\n",
    "\n",
    "对于一个确定的样本，$\\text{SST}$ 是固定的，$\\text{SSR}$ 越大，说明回归方程能越好地解释原数据的变异；同时 $\\text{SSE}$ 越小，说明回归方程对原数据拟合得越好。\n",
    "\n",
    "定义**判定系数**（coefficient of determination）：\n",
    "\n",
    "$$\n",
    "R^2=\\frac{\\text{SSR}}{\\text{SST}}=1-\\frac{\\text{SSE}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "可以知道 $R^2\\in[0,1]$，且其数值越大，表明拟合得越好；当 $R^2=1$ 时，拟合点与原数据完全吻合；若预测函数为常值函数（即始终预测 $\\bar y$），那么 $R^2=0$，因为它完全不能解释原数据的变异。\n",
    "\n",
    "可以证明，$\\sqrt{R^2}$ 等于 $x,y$ 的相关系数绝对值，其符号与 $\\hat\\beta_1$ 相同。\n",
    "\n",
    "\n",
    "\n",
    "### 1.4 显著性检验\n",
    "\n",
    "现在我们给出了 $x$ 和 $y$ 的线性关系，但是这基于一个假设：$x$ 的变化确实会影响到 $y$ 的变化，这个假设是否真实还需要检验。如果 $x$ 对 $y$ 没有显著影响，相当于假设：$H_0:\\beta_1=0$，则在该假设下有：\n",
    "\n",
    "$$\n",
    "F=\\frac{\\text{SSR}/1}{\\text{SSE}/(n-2)}\\sim F(1, n-2)\n",
    "$$\n",
    "\n",
    "于是我们可以对该统计量做 F 检验。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64886698",
   "metadata": {},
   "source": [
    "## 2 多元线性回归\n",
    "\n",
    "### 2.1 模型\n",
    "\n",
    "多元回归分析的模型为：\n",
    "\n",
    "$$\n",
    "y=\\beta_0+\\beta_1x_1+\\cdots+\\beta_mx_m+\\epsilon\n",
    "$$\n",
    "\n",
    "其中，$\\beta_0,\\ldots, \\beta_m$ 是回归系数，$\\epsilon\\sim N(0, \\sigma^2)$ 是随机误差项。\n",
    "\n",
    "设我们进行了 $n$ 次观测，得到数据：$(y_i, x_{i1},\\ldots,x_{im}),\\,i=1,2,\\ldots,n$，则：\n",
    "\n",
    "$$\n",
    "y_i=\\beta_0+\\beta_1x_{i1}+\\cdots+\\beta_mx_{im}+\\epsilon_i,\\,i=1,2,\\ldots,n\n",
    "$$\n",
    "\n",
    "若令：\n",
    "\n",
    "$$\n",
    "Y=\\begin{bmatrix}y_1\\\\\\vdots\\\\y_n\\end{bmatrix},\\,\n",
    "X=\\begin{bmatrix}\n",
    "1&x_{11}&\\cdots&x_{1m}\\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "1&x_{n1}&\\cdots&x_{nm}\n",
    "\\end{bmatrix},\\,\n",
    "\\epsilon=\\begin{bmatrix}\\epsilon_1\\\\\\vdots\\\\\\epsilon_n\\end{bmatrix},\\,\n",
    "\\beta=\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_m\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "那么上式可以表示为：\n",
    "\n",
    "$$\n",
    "Y=X\\beta+\\epsilon\n",
    "$$\n",
    "\n",
    "### 2.2 最小二乘估计\n",
    "\n",
    "仍然使用最小二乘法，使得误差平方和最小：\n",
    "\n",
    "$$\n",
    "\\hat\\beta=\\arg\\min_\\beta\\sum_{i=1}^n(y_i-\\beta\\cdot x_i)^2\n",
    "$$\n",
    "\n",
    "可以解得（若 $X$ 满秩）：\n",
    "\n",
    "$$\n",
    "\\hat\\beta=(X^TX)^{-1}X^TY\n",
    "$$\n",
    "\n",
    "于是数据的拟合值为 $\\hat Y=X\\hat\\beta$，残差为 $e=Y-\\hat Y$，残差平方和为 $\\text{SSE}=\\sum\\limits_{i=1}^ne_i^2=\\sum\\limits_{i=1}^n(y_i-\\hat y_i)^2$\n",
    "\n",
    "### 2.3 统计分析\n",
    "\n",
    "1. $\\hat\\beta$ 是 $\\beta$ 的线性无偏最小方差估计；\n",
    "2. $\\beta\\sim N(\\beta, \\sigma^2(X^TX)^{-1})$\n",
    "3. $\\mathbb E[\\text{SSE}]=(n-m-1)\\sigma^2$，$\\frac{\\text{SSE}}{\\sigma^2}\\sim \\chi^2(n-m-1)$，由此得到 $\\sigma^2$ 的无偏估计：$s^2=\\frac{\\text{SSE}}{n-m-1}=\\hat\\sigma^2$，称 $s^2$ 为剩余方差，$s$ 为剩余标准差；\n",
    "4. $\\text{SST}=\\text{SSE}+\\text{SSR}$，其中 $\\text{SSR}=\\sum\\limits_{i=1}^n(\\hat y-\\bar y)^2$；\n",
    "5. 判定系数：$R^2=\\frac{\\text{SSR}}{\\text{SST}}=1-\\frac{\\text{SSE}}{\\text{SST}}$\n",
    "\n",
    "### 2.4 假设检验\n",
    "\n",
    "令原假设 $H_0:\\beta_1=\\cdots=\\beta_m=0$，那么假设成立时有：\n",
    "\n",
    "$$\n",
    "F=\\frac{\\text{SSR}/m}{\\text{SSE}/(n-m-1)}\\sim F(m, n-m-1)\n",
    "$$\n",
    "\n",
    "于是我们可以对该统计量做 F 检验。\n",
    "\n",
    "<br>\n",
    "\n",
    "但是当上述 $H_0$ 被拒绝时，只能说明 $\\beta_j$ 不全为零，不能排除其中若干个为零。如果要对每一个 $\\beta_j$ 进行判断，应该作下述 $m+1$ 个 $t$ 检验：\n",
    "\n",
    "设原假设 $H_0^{(j)}:\\beta_j=0$，则当 $H_0^{(j)}$ 成立时，有：\n",
    "\n",
    "$$\n",
    "t_j=\\frac{\\hat\\beta_j/\\sqrt{c_{jj}}}{\\sqrt{\\text{SSE}/(n-m-1)}}\\sim t(n-m-1)\n",
    "$$\n",
    "\n",
    "其中，$c_{jj}$ 是 $(X^TX)^{-1}$ 中第 $(j, j)$ 元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1c3c2",
   "metadata": {},
   "source": [
    "## 3 变量筛选\n",
    "\n",
    "待续"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276f67f",
   "metadata": {},
   "source": [
    "## 4 代码\n",
    "\n",
    "`sklearn.linear_model.LinearRegression` 提供了线性回归的类，支持拟合数据、预测数据、输出 $R^2$ 判定系数。\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "<br>\n",
    "\n",
    "为了功能的完整性，我在同文件夹下 `statistics.py` 模块中封装了 `RegressionAnalysis` 类，在 `LinearRegression` 基础上增加了 F 检验、t 检验的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f400ccd",
   "metadata": {},
   "source": [
    "## 5 例题\n",
    "\n",
    "### 5.1 例一\n",
    "\n",
    "| $x_1$ | 120  | 140  | 190  | 130  | 155  | 175  | 125  | 145  | 180  | 150  |\n",
    "| ----- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| $x_2$ | 100  | 110  | 90   | 150  | 210  | 150  | 250  | 270  | 300  | 250  |\n",
    "| $y$   | 102  | 100  | 120  | 77   | 46   | 93   | 26   | 69   | 65   | 85   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8a840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = np.array([120, 140, 190, 130, 155, 175, 125, 145, 180, 150])\n",
    "x2 = np.array([100, 110, 90, 150, 210, 150, 250, 270, 300, 250])\n",
    "y = np.array([102, 100, 120, 77, 46, 93, 26, 69, 65, 85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6685cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef:\t [66.51756832  0.41391526 -0.2697807 ]\n",
      "R2:\t 0.6527308195492921\n",
      "MSE:\t 351.0444925410363\n",
      "f_test:\t 0.024679243751388857\n",
      "t_test:\t [0.07810956 0.0779739  0.9937435 ]\n"
     ]
    }
   ],
   "source": [
    "from statistics import RegressionAnalysis\n",
    "\n",
    "x = np.hstack((x1.reshape(-1, 1), x2.reshape(-1, 1)))\n",
    "reg = RegressionAnalysis(x, y)\n",
    "print('coef:\\t', reg.coef)\n",
    "print('R2:\\t', reg.R2)\n",
    "print('MSE:\\t', reg.MSE)\n",
    "print('f_test:\\t', reg.f_test())\n",
    "print('t_test:\\t', reg.t_test())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
